{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Jigsaws.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "0bH6mXyGaLgD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "from tensorflow.nn import softmax_cross_entropy_with_logits_v2 as categorical_cross_entropy\n",
        "from tensorflow.contrib.layers import conv2d, max_pool2d, fully_connected, flatten, dropout\n",
        "#import tensorflow.nn.softmax\n",
        "from tensorflow.losses import softmax_cross_entropy\n",
        "from tensorflow.train import AdamOptimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vZbsQb83ar8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        },
        "outputId": "048962cd-d143-43d0-8754-778d2795ceb5"
      },
      "cell_type": "code",
      "source": [
        "data = np.load('test.npy')\n",
        "data = data[()]\n",
        "train_x = np.array(data['Data'])\n",
        "train_y = np.array(data['Class_Value'])\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "\n",
        "samples = train_x\n",
        "for i in range(len(train_x)):\n",
        "   samples[i] = np.array(train_x[i])\n",
        "   print(samples[i].shape)\n",
        "print(samples[2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40,)\n",
            "(40,)\n",
            "(54, 76)\n",
            "(60, 76)\n",
            "(119, 76)\n",
            "(72, 76)\n",
            "(129, 76)\n",
            "(84, 76)\n",
            "(68, 76)\n",
            "(81, 76)\n",
            "(63, 76)\n",
            "(95, 76)\n",
            "(79, 76)\n",
            "(63, 76)\n",
            "(88, 76)\n",
            "(39, 76)\n",
            "(74, 76)\n",
            "(108, 76)\n",
            "(102, 76)\n",
            "(90, 76)\n",
            "(108, 76)\n",
            "(60, 76)\n",
            "(68, 76)\n",
            "(30, 76)\n",
            "(87, 76)\n",
            "(150, 76)\n",
            "(101, 76)\n",
            "(110, 76)\n",
            "(54, 76)\n",
            "(71, 76)\n",
            "(75, 76)\n",
            "(68, 76)\n",
            "(76, 76)\n",
            "(83, 76)\n",
            "(115, 76)\n",
            "(99, 76)\n",
            "(110, 76)\n",
            "(97, 76)\n",
            "(84, 76)\n",
            "(54, 76)\n",
            "(64, 76)\n",
            "(95, 76)\n",
            "[['0.15376700' '-0.05356000' '0.30664900' ... '-0.74965000' '-0.35224000'\n",
            "  '-0.43486200']\n",
            " ['0.15392500' '-0.05260600' '0.30505000' ... '-0.86647100' '-0.62153000'\n",
            "  '-0.44204700']\n",
            " ['0.15393900' '-0.05197800' '0.30414900' ... '-0.78757800' '-0.39792400'\n",
            "  '-0.42882000']\n",
            " ...\n",
            " ['0.15983700' '-0.04261300' '0.29065300' ... '0.00144900' '-0.24994600'\n",
            "  '-0.12020100']\n",
            " ['0.16114200' '-0.04068200' '0.28910400' ... '-0.06404700' '-0.52653600'\n",
            "  '-0.10354500']\n",
            " ['0.16246700' '-0.03883900' '0.28764300' ... '-0.55719200' '-0.57501600'\n",
            "  '-0.05733400']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2InMa-YTaQu0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Network(object):\n",
        "  def __init__ (self, trainer):\n",
        "    self.X = tf.placeholder(tf.float32, shape = [None, 60, 76]) # W x C where W is length and C is channels\n",
        "    self.Y = tf.placeholder(tf.int64, shape= [None, 3])\n",
        "    X_r = tf.reshape(self.X, [-1, 60, 76, 1]) \n",
        "    Y_r = tf.reshape(self.Y, [-1, 3])\n",
        "    C1 = tf.contrib.layers.conv2d(inputs=X_r, num_outputs=38, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M1 = tf.contrib.layers.max_pool2d(inputs=C1, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D1 = tf.contrib.layers.dropout(inputs=M1, keep_prob=0.8)\n",
        "    C2 = tf.contrib.layers.conv2d(inputs=D1, num_outputs=76, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M2 = tf.contrib.layers.max_pool2d(inputs=C2, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D2 = tf.contrib.layers.dropout(inputs=M2, keep_prob=0.8)\n",
        "    C3 = tf.contrib.layers.conv2d(inputs=D2, num_outputs=152, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M3 = tf.contrib.layers.max_pool2d(inputs=C3, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D2 = tf.contrib.layers.dropout(inputs=M3, keep_prob=0.8)\n",
        "    \n",
        "    F1 = fully_connected(inputs=tf.contrib.layers.flatten(D2), num_outputs=64, activation_fn = tf.nn.softmax)\n",
        "    FD1 = tf.contrib.layers.dropout(inputs=F1, keep_prob=0.5)\n",
        "    F2 = fully_connected(inputs=FD1, num_outputs = 32)\n",
        "    FD2 = tf.contrib.layers.dropout(inputs=F2, keep_prob=0.5)\n",
        "    self.Out = fully_connected(inputs=FD2, num_outputs = 3, activation_fn = tf.nn.softmax)\n",
        "    self.loss = tf.losses.softmax_cross_entropy(Y_r, self.Out)\n",
        "    \n",
        "    local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "    self.gradients = tf.gradients(self.loss, local_vars)\n",
        "    self.apply_grads = trainer.apply_gradients(zip(self.gradients, local_vars))\n",
        "    \n",
        "    self.accuracy =  tf.reduce_mean(tf.cast(\n",
        "            tf.equal(tf.argmax(self.Out, 1), tf.argmax(self.Y, 1)), \n",
        "            tf.float32))\n",
        "  \n",
        "opt = AdamOptimizer()\n",
        "net = Network(opt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrWdJVzXcLY3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "62c7c62f-1054-4fa6-801b-6a36c159e14e"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "    training_epochs = 50\n",
        "    display_step = 1\n",
        "    # Fit all training data\n",
        "    for epoch in range(training_epochs):\n",
        "      sess.run(net.apply_grads, feed_dict={net.X: train_X, net.Y: train_Y})\n",
        "\n",
        "        # Display logs per epoch step\n",
        "      if (epoch+1) % display_step == 0:\n",
        "        c = sess.run(net.loss, feed_dict={net.X: train_X, net.Y: train_Y})\n",
        "        acc = sess.run(net.accuracy, feed_dict={net.X: train_X, net.Y: train_Y})\n",
        "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{%f}\" % c, \"accuracy=\", \"{%f}\" % acc) \n",
        "            # \\\"W=\", sess.run(W), \"b=\", sess.run(b))\n",
        "\n",
        "    print(\"Optimization Finished!\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1623e34dee98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Fit all training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Display logs per epoch step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_X' is not defined"
          ]
        }
      ]
    }
  ]
}