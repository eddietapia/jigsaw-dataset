{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Augmentation and CNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "TEr29PLrSBh0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Project: Data Preparation & Inputs\n"
      ]
    },
    {
      "metadata": {
        "id": "Z9h6YsRTY_75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. z-normalization\n",
        "Each channel of raw data, x, is normalized individually as z = (x−µ)/σ, where µ and α are the mean and standard deviation of vector x"
      ]
    },
    {
      "metadata": {
        "id": "Ajtf_GWA2U55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def zNorm(data):\n",
        "  #for i in range(len(train_x_ts_data)):\n",
        "  Sums = [];\n",
        "  Avgs = [];\n",
        "  Variances = [];\n",
        "  SDevs = [];\n",
        "  NumEles=0;\n",
        "\n",
        "  for x in range(76):\n",
        "    Sums.append(0);\n",
        "    Avgs.append(0);\n",
        "    Variances.append(0);\n",
        "    SDevs.append(0);\n",
        "\n",
        "  #data = [[[1,2,3,4],[76,55,44,21]],[[93,46,102,75],[74,33,51,935]],[[0,214,241,122],[150,151,251,500]]]\n",
        "\n",
        "\n",
        "  #for each of 40 trials\n",
        "  for TrialNum in range(len(data)):\n",
        "    trial=data[TrialNum];\n",
        "    #for time step 1 to 54\n",
        "    for TimeStepNum in range(len(trial)):\n",
        "      timeStep=trial[TimeStepNum];\n",
        "      NumEles+=1;\n",
        "      #for feature 1 to 76\n",
        "      for FeatureNum in range(len(timeStep)):\n",
        "        #add to sum\n",
        "        Sums[FeatureNum]+=((float)(timeStep[FeatureNum])); #https://stackoverflow.com/questions/379906/how-do-i-parse-a-string-to-a-float-or-int-in-python\n",
        "\n",
        "  for x in range (len(Sums)):\n",
        "    Avgs[x]=Sums[x]/NumEles;\n",
        "\n",
        "  #for each of 40 trials\n",
        "  for TrialNum in range(len(data)):\n",
        "    trial=data[TrialNum];\n",
        "    #for time step 1 to 54\n",
        "    for TimeStepNum in range(len(trial)):\n",
        "      timeStep=trial[TimeStepNum];\n",
        "      #for feature 1 to 76\n",
        "      for FeatureNum in range(len(timeStep)):\n",
        "        #add to devs\n",
        "        Variances[FeatureNum]+=(((float)(timeStep[FeatureNum]))-Avgs[FeatureNum])*(((float)(timeStep[FeatureNum]))-Avgs[FeatureNum]);\n",
        "\n",
        "  #divide by lens\n",
        "  for x in range (len(Variances)):\n",
        "    SDevs[x]=np.sqrt(Variances[x]/NumEles);  #https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html\n",
        "  newData=[];\n",
        "  for x in range(len(data)):\n",
        "    newData.append(data[x]); #https://stackoverflow.com/questions/6431973/how-to-copy-data-from-a-numpy-array-to-another\n",
        "\n",
        "  #for each of 40 trials\n",
        "  for TrialNum in range(len(data)):\n",
        "    trial=newData[TrialNum];\n",
        "    #for time step 1 to 54\n",
        "    for TimeStepNum in range(len(trial)):\n",
        "      timeStep=trial[TimeStepNum];\n",
        "      #for feature 1 to 76\n",
        "      for FeatureNum in range(len(timeStep)):\n",
        "        #normalize as you're supposed to\n",
        "        timeStep[FeatureNum] = (((float)(timeStep[FeatureNum])) - Avgs[FeatureNum])/SDevs[FeatureNum]\n",
        "    return newData;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0UnNtz5ZDPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Data augmentation with a sliding window"
      ]
    },
    {
      "metadata": {
        "id": "yLKN60dXSKAY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## jigDataSplit\n",
        "\n",
        "**\n",
        "** Splits a 2D numpy array of the form [[Data],[Classification]] into a 2D array of [[Windows],[Classification]] \n",
        "\n",
        "@parameter train_x: the z-normalized raw data\n",
        "\n",
        "@parameter train_y: the original classifications (0,1,2)\n",
        "\n",
        "@parameter stepSize: the amount we shift the window by each time\n",
        "\n",
        "\n",
        "@parameter windowSize: the size of the window that we want for the time\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "b8LJaVqLR400",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def jigDataSplit(train_x, train_y, stepSize, windowSize):\n",
        "  train_x_new = []\n",
        "  train_y_new = []\n",
        "  #n = count of new trials\n",
        "  n = 0\n",
        "  #for each of the 40 trials\n",
        "  for i in range(40):\n",
        "    #m = the index of the row we are at within the current trial\n",
        "    m = 0\n",
        "    #check if we have reached the end of the rows for the trial\n",
        "    while (m + windowSize < len(train_x[i])):\n",
        "      #create a new trial of length windowSize\n",
        "      new_trial = []\n",
        "      for j in range(windowSize):\n",
        "        new_trial.append(train_x[i][j])\n",
        "      #add new trial to the new testing dataset\n",
        "      train_x_new.append(new_trial)\n",
        "      #classify the new trial with the pre-existing classification\n",
        "      train_y_new.append(train_y[i])\n",
        "      m = m + stepSize\n",
        "      n = n + 1\n",
        "    #add a trial for the rest of the data padded by zeros\n",
        "    zero_padded_trial = []\n",
        "    #add rest of original data\n",
        "    for j in range(m, len(train_x[i])):\n",
        "      zero_padded_trial.append(train_x[i][j])\n",
        "    #add zeros padding\n",
        "    for j in range (len(train_x[i]), m + windowSize):\n",
        "      zero_padded_trial.append([0]*76)\n",
        "    train_x_new.append(zero_padded_trial)\n",
        "    train_y_new.append(train_y[i])\n",
        "    n = n + 1\n",
        "  samples_new = train_x_new\n",
        "  for i in range(len(train_x_new)):\n",
        "    samples_new[i] = np.array(train_x_new[i])\n",
        "    print(samples_new[i].shape)\n",
        "  samples_new = np.array(samples_new)\n",
        "  classifications_new = np.array(train_y_new)\n",
        "  return (samples_new, classifications_new)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1MegKuhSKU4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Split the data between master-side and patient-side manipulators\n",
        "\n",
        "**\n",
        "**\n",
        "Split a 2D numpy array of the form [[Windows],[Classification]] into a list containing two 2D numpy arrays, one for master-side data and one for patient-side data\n",
        "\n",
        "Data from each row of length 76 is split into two rows each of length 38\n"
      ]
    },
    {
      "metadata": {
        "id": "7nXgUWuIDAfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def splitMasterPatient(train_x, train_y, windowSize):\n",
        "  new_train_x = []\n",
        "  for window in train_x:\n",
        "    window_split = np.hsplit(window, 2)\n",
        "    new_train_x.append(window_split)\n",
        "  return (new_train_x)\n",
        "  new_train_y = []\n",
        "  for classification in train_y:\n",
        "    new_train_y.append([classicication, classification])\n",
        "  master_data = []\n",
        "  master_classes = []\n",
        "  patient_data = []\n",
        "  patient_classes = []\n",
        "  for i in range(len(new_train_x)):\n",
        "    master_data.append(new_train_x[i][0])\n",
        "    master_classes.append(new_train_y[i][0])\n",
        "    patient_data.append(new_train_x[i][1])\n",
        "    patient_classes.append(new_train_y[i][1])\n",
        "  master_data = np.array(master_data)\n",
        "  master_classes = np.array(master_classes)\n",
        "  patient_data = np.array(patient_data)\n",
        "  patient_classes = np.array(patient_classes)\n",
        "  return master_data + patient_data\n",
        "  #return [[master_data, master_classes], [patient_data, patient_classes]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Lnf7WM22c3X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Main"
      ]
    },
    {
      "metadata": {
        "id": "JlIfHdNAZCQR",
        "colab_type": "code",
        "outputId": "e21b6d8e-fc5d-4299-df03-c0925bb12ec6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1589
        }
      },
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle as pkl\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import stats\n",
        "from tempfile import TemporaryFile\n",
        "\n",
        "data = np.load('test.npy')\n",
        "\n",
        "# Convert numpy object back to dictionary\n",
        "data = data[()]\n",
        "# Split data for classification\n",
        "train_x_ts_data = np.array(data['Data'])\n",
        "train_y_classification = np.array(data['Class_Value'])\n",
        "#print('Features', train_x_ts_data.shape)\n",
        "#print('Class_Value', train_y_classification.shape)\n",
        "\n",
        "#printing out what the shape of the entire input is\n",
        "#samples = train_x_ts_data\n",
        "#for i in range(len(train_x_ts_data)):\n",
        "#   samples[i] = np.array(train_x_ts_data[i])\n",
        "#   print(samples[i].shape)\n",
        "#print('SAMPLE[0][0]',train_x_ts_data[0][0])\n",
        "#print('Class[0]',train_y_classification[0])\n",
        "\n",
        "#take data, Znorm\n",
        "#split into windows\n",
        "#save as npy object\n",
        "\n",
        "\n",
        "\n",
        "#print('Trials[0][0]',data[0][0])\n",
        "#print('Class[0]',train_y_classification[0])\n",
        "#print('Avgs[0]',Avgs[0])\n",
        "#print('NewData[0]',zNorm([[[1,2,3,4],[76,55,44,21]],[[93,46,102,75],[74,33,51,935]],[[0,214,241,122],[150,151,251,500]]]))\n",
        "#print('SDevs[0]',SDevs[0])\n",
        "#print('NewData:',newData)\n",
        "#print(train_y_classification)\n",
        "\n",
        "\n",
        "newX = zNorm(train_x_ts_data);\n",
        "(x,y) = jigDataSplit(newX, train_y_classification, 30, 60)\n",
        "\n",
        "\n",
        "\n",
        "#print('Features', x.shape)\n",
        "#print('Class_Value', y.shape)\n",
        "#print(y)\n",
        "\n",
        "#save (x,y) tuple into .npy objects\n",
        "outFile1 = TemporaryFile()\n",
        "np.save('output1_x', x)\n",
        "\n",
        "outFile2 = TemporaryFile()\n",
        "np.save('output1_y', y)\n",
        "\n",
        "#master = splitMasterPatient(x, y, 60)\n",
        "#print(master)\n",
        "#print(y)\n",
        "\n",
        "#https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "Features (89, 60, 76)\n",
            "Class_Value (89,)\n",
            "[1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uIQFxIm-Dzkl",
        "colab_type": "code",
        "outputId": "b86befcb-f2df-4026-c175-7c1968e76db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow.nn\n",
            "\u001b[31m  Could not find a version that satisfies the requirement tensorflow.nn (from versions: )\u001b[0m\n",
            "\u001b[31mNo matching distribution found for tensorflow.nn\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ErzTGEP0J_OJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "from tensorflow.nn import softmax_cross_entropy_with_logits_v2 as categorical_cross_entropy\n",
        "from tensorflow.contrib.layers import conv2d, max_pool2d, fully_connected, flatten, dropout\n",
        "#import tensorflow.nn.softmax\n",
        "from tensorflow.losses import softmax_cross_entropy\n",
        "from tensorflow.train import AdamOptimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NovWPxQvKB_I",
        "colab_type": "code",
        "outputId": "4cc9e995-a1bc-4cff-a2c3-6e300188a9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1756
        }
      },
      "cell_type": "code",
      "source": [
        "data_x = x\n",
        "data_y =y \n",
        "train_x = np.array(data_x)\n",
        "train_y = np.array(data_y)\n",
        "#train_y = np.transpose(train_y)\n",
        "\n",
        "#data = data[()]\n",
        "#train_x = np.array(data['Features'])\n",
        "#train_y = np.array(data['Class_Value'])\n",
        "#print(train_x.shape)\n",
        "#print(train_y.shape)\n",
        "\n",
        "\n",
        "samples = train_x\n",
        "for i in range(len(train_x)):\n",
        "   samples[i] = np.array(train_x[i])\n",
        "   #print(samples[i].shape)\n",
        "#print(samples[2])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(89, 60, 76)\n",
            "(89,)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "[['0.15376700' '-0.05356000' '0.30664900' ... '-0.74965000' '-0.35224000'\n",
            "  '-0.43486200']\n",
            " ['0.15392500' '-0.05260600' '0.30505000' ... '-0.86647100' '-0.62153000'\n",
            "  '-0.44204700']\n",
            " ['0.15393900' '-0.05197800' '0.30414900' ... '-0.78757800' '-0.39792400'\n",
            "  '-0.42882000']\n",
            " ...\n",
            " ['0.15489500' '-0.04978900' '0.29845800' ... '-0.00001200' '-0.00019800'\n",
            "  '-0.22013500']\n",
            " ['0.15501600' '-0.04979400' '0.29848500' ... '-0.10985600' '0.16849200'\n",
            "  '-0.23270800']\n",
            " ['0.15505100' '-0.04979300' '0.29847000' ... '-0.00067000' '-0.00027700'\n",
            "  '-0.23744300']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LyCdstAzHwg5",
        "colab_type": "code",
        "outputId": "ff9f765d-e3a7-4402-8de0-f1ad8cfeaec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-20 19:28:45--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 3.92.108.98, 34.206.253.53, 34.196.237.103, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|3.92.108.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  14.28M  29.1MB/s    in 0.5s    \n",
            "\n",
            "2019-04-20 19:28:45 (29.1 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "https://24381b43.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IKvJGw6TKUvW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "CNN based from \"Deep LEarning with Convolutional Neural Network for Objective Skill Evaluation\n",
        "in Robot-assisted Surgery\" by Ziheng Wang and Ann Majewicz Fey\n",
        "10 January 2018 \n",
        "arXiv: 1806.05796v1\n",
        "'''\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class CNN_Wang_Fey(object):\n",
        "  def __init__ (self, trainer):\n",
        "    self.X = tf.placeholder(tf.float32, shape = [None, None, None])#60, 76]) # W x C where W is length and C is channels\n",
        "    self.Y = tf.placeholder(tf.int64, shape= [None,])\n",
        "    MSM_X = tf.slice(self.X, [0, 0, 0], [-1, 60, 38])\n",
        "    PSM_X = tf.slice(self.X, [0, 0, 38], [-1, 60, 38])\n",
        "    X_r = tf.reshape(PSM_X, [-1, 60, 38, 1]) \n",
        "    #Y_r = tf.reshape(self.Y, [-1, 1])\n",
        "    Y_one_hot = tf.one_hot(self.Y, 3)\n",
        "    C1 = tf.contrib.layers.conv2d(inputs=X_r, num_outputs=38, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M1 = tf.contrib.layers.max_pool2d(inputs=C1, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D1 = tf.contrib.layers.dropout(inputs=M1, keep_prob=0.8)\n",
        "    C2 = tf.contrib.layers.conv2d(inputs=D1, num_outputs=76, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M2 = tf.contrib.layers.max_pool2d(inputs=C2, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D2 = tf.contrib.layers.dropout(inputs=M2, keep_prob=0.8)\n",
        "    C3 = tf.contrib.layers.conv2d(inputs=D2, num_outputs=152, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M3 = tf.contrib.layers.max_pool2d(inputs=C3, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D2 = tf.contrib.layers.dropout(inputs=M3, keep_prob=0.8)\n",
        "    \n",
        "    F1 = fully_connected(inputs=tf.contrib.layers.flatten(D2), num_outputs=64, activation_fn = tf.nn.softmax)\n",
        "    FD1 = tf.contrib.layers.dropout(inputs=F1, keep_prob=0.5)\n",
        "    F2 = fully_connected(inputs=FD1, num_outputs = 32)\n",
        "    FD2 = tf.contrib.layers.dropout(inputs=F2, keep_prob=0.5)\n",
        "    self.Out = fully_connected(inputs=FD2, num_outputs = 3, activation_fn = tf.nn.softmax)\n",
        "    \n",
        "    tf.summary.histogram(\"softmax\", self.Out)\n",
        "    self.loss = tf.losses.softmax_cross_entropy(Y_one_hot, self.Out)\n",
        "    tf.summary.scalar('loss', self.loss)\n",
        "    local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "    self.gradients = tf.gradients(self.loss, local_vars)\n",
        "    self.apply_grads = trainer.apply_gradients(zip(self.gradients, local_vars))\n",
        "    \n",
        "    self.accuracy =  tf.reduce_mean(tf.cast(\n",
        "            tf.equal(tf.argmax(self.Out, 1), tf.argmax(Y_one_hot, 1)), \n",
        "            tf.float32))\n",
        "    tf.summary.scalar('accuracy', self.accuracy)\n",
        "\n",
        "  \n",
        "opt = AdamOptimizer()\n",
        "net = CNN_Wang_Fey(opt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4XHTb7WoKZMM",
        "colab_type": "code",
        "outputId": "1f2c558f-5959-4f7f-f34a-5ccc47d2b928",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.Session() as sess:\n",
        "    merge = tf.summary.merge_all()\n",
        "    train_writer = tf.summary.FileWriter( './log', sess.graph)\n",
        "    \n",
        "    # Run the initializer\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "   \n",
        "    training_epochs = 10\n",
        "    display_step = 1\n",
        "    # Fit all training data\n",
        "    for epoch in range(training_epochs):\n",
        "      \n",
        "      summary, _ = sess.run([merge, net.apply_grads], feed_dict={net.X: train_x, net.Y: train_y})\n",
        "      train_writer.add_summary(summary, epoch)\n",
        "        # Display logs per epoch step\n",
        "      if (epoch+1) % display_step == 0:\n",
        "        c = sess.run(net.loss, feed_dict={net.X: train_x, net.Y: train_y})\n",
        "        #print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{%f}\" % c) \n",
        "        acc = sess.run(net.accuracy, feed_dict={net.X: train_x, net.Y: train_y})\n",
        "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{%f}\" % c, \"accuracy=\", \"{%f}\" % acc) \n",
        "            # \\\"W=\", sess.run(W), \"b=\", sess.run(b))\n",
        "\n",
        "    print(\"Optimization Finished!\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= {1.096561} accuracy= {0.438202}\n",
            "Epoch: 0002 cost= {1.094386} accuracy= {0.426966}\n",
            "Epoch: 0003 cost= {1.090875} accuracy= {0.539326}\n",
            "Epoch: 0004 cost= {1.087595} accuracy= {0.516854}\n",
            "Epoch: 0005 cost= {1.087264} accuracy= {0.528090}\n",
            "Epoch: 0006 cost= {1.081335} accuracy= {0.516854}\n",
            "Epoch: 0007 cost= {1.082471} accuracy= {0.494382}\n",
            "Epoch: 0008 cost= {1.074433} accuracy= {0.494382}\n",
            "Epoch: 0009 cost= {1.066763} accuracy= {0.505618}\n",
            "Epoch: 0010 cost= {1.080781} accuracy= {0.505618}\n",
            "Optimization Finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SBfTTkDzjZ1B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}