{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Augmentation and CNN",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etapiahe/jigsaw-dataset/blob/master/Data_Augmentation_and_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TEr29PLrSBh0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Project: Data Preparation & Inputs\n"
      ]
    },
    {
      "metadata": {
        "id": "Z9h6YsRTY_75",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. z-normalization\n",
        "Each channel of raw data, x, is normalized individually as z = (x−µ)/σ, where µ and α are the mean and standard deviation of vector x"
      ]
    },
    {
      "metadata": {
        "id": "Ajtf_GWA2U55",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def zNorm(data):\n",
        "  #for i in range(len(train_x_ts_data)):\n",
        "  Sums = [];\n",
        "  Avgs = [];\n",
        "  Variances = [];\n",
        "  SDevs = [];\n",
        "  NumEles=0;\n",
        "\n",
        "  for x in range(76):\n",
        "    Sums.append(0);\n",
        "    Avgs.append(0);\n",
        "    Variances.append(0);\n",
        "    SDevs.append(0);\n",
        "\n",
        "  #data = [[[1,2,3,4],[76,55,44,21]],[[93,46,102,75],[74,33,51,935]],[[0,214,241,122],[150,151,251,500]]]\n",
        "\n",
        "\n",
        "  #for each of 40 trials\n",
        "  for TrialNum in range(len(data)):\n",
        "    trial=data[TrialNum];\n",
        "    #for time step 1 to 54\n",
        "    for TimeStepNum in range(len(trial)):\n",
        "      timeStep=trial[TimeStepNum];\n",
        "      NumEles+=1;\n",
        "      #for feature 1 to 76\n",
        "      for FeatureNum in range(len(timeStep)):\n",
        "        #add to sum\n",
        "        Sums[FeatureNum]+=((float)(timeStep[FeatureNum])); #https://stackoverflow.com/questions/379906/how-do-i-parse-a-string-to-a-float-or-int-in-python\n",
        "\n",
        "  for x in range (len(Sums)):\n",
        "    Avgs[x]=Sums[x]/NumEles;\n",
        "\n",
        "  #for each of 40 trials\n",
        "  for TrialNum in range(len(data)):\n",
        "    trial=data[TrialNum];\n",
        "    #for time step 1 to 54\n",
        "    for TimeStepNum in range(len(trial)):\n",
        "      timeStep=trial[TimeStepNum];\n",
        "      #for feature 1 to 76\n",
        "      for FeatureNum in range(len(timeStep)):\n",
        "        #add to devs\n",
        "        Variances[FeatureNum]+=(((float)(timeStep[FeatureNum]))-Avgs[FeatureNum])*(((float)(timeStep[FeatureNum]))-Avgs[FeatureNum]);\n",
        "\n",
        "  #divide by lens\n",
        "  for x in range (len(Variances)):\n",
        "    SDevs[x]=np.sqrt(Variances[x]/NumEles);  #https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html\n",
        "  newData=[];\n",
        "  for x in range(len(data)):\n",
        "    newData.append(data[x]); #https://stackoverflow.com/questions/6431973/how-to-copy-data-from-a-numpy-array-to-another\n",
        "\n",
        "  #for each of 40 trials\n",
        "  for TrialNum in range(len(data)):\n",
        "    trial=newData[TrialNum];\n",
        "    #for time step 1 to 54\n",
        "    for TimeStepNum in range(len(trial)):\n",
        "      timeStep=trial[TimeStepNum];\n",
        "      #for feature 1 to 76\n",
        "      for FeatureNum in range(len(timeStep)):\n",
        "        #normalize as you're supposed to\n",
        "        timeStep[FeatureNum] = (((float)(timeStep[FeatureNum])) - Avgs[FeatureNum])/SDevs[FeatureNum]\n",
        "    return newData;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t0UnNtz5ZDPK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. Data augmentation with a sliding window"
      ]
    },
    {
      "metadata": {
        "id": "yLKN60dXSKAY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## jigDataSplit\n",
        "\n",
        "**\n",
        "** Splits a 2D numpy array of the form [[Data],[Classification]] into a 2D array of [[Windows],[Classification]] \n",
        "\n",
        "@parameter train_x: the z-normalized raw data\n",
        "\n",
        "@parameter train_y: the original classifications (0,1,2)\n",
        "\n",
        "@parameter stepSize: the amount we shift the window by each time\n",
        "\n",
        "\n",
        "@parameter windowSize: the size of the window that we want for the time\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "b8LJaVqLR400",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def jigDataSplit(train_x, train_y, stepSize, windowSize, num_trials):\n",
        "  train_x_new = []\n",
        "  train_y_new = []\n",
        "  #n = count of new trials\n",
        "  n = 0\n",
        "  #for each of the trials\n",
        "  for i in range(num_trials):\n",
        "    #m = the index of the row we are at within the current trial\n",
        "    m = 0\n",
        "    #check if we have reached the end of the rows for the trial\n",
        "    while (m + windowSize < len(train_x[i])):\n",
        "      #create a new trial of length windowSize\n",
        "      new_trial = []\n",
        "      for j in range(windowSize):\n",
        "        new_trial.append(train_x[i][j])\n",
        "      #add new trial to the new testing dataset\n",
        "      train_x_new.append(new_trial)\n",
        "      #classify the new trial with the pre-existing classification\n",
        "      train_y_new.append(train_y[i])\n",
        "      m = m + stepSize\n",
        "      n = n + 1\n",
        "    #add a trial for the rest of the data padded by zeros\n",
        "    zero_padded_trial = []\n",
        "    #add rest of original data\n",
        "    for j in range(m, len(train_x[i])):\n",
        "      zero_padded_trial.append(train_x[i][j])\n",
        "    #add zeros padding\n",
        "    for j in range (len(train_x[i]), m + windowSize):\n",
        "      zero_padded_trial.append([0]*76)\n",
        "    train_x_new.append(zero_padded_trial)\n",
        "    train_y_new.append(train_y[i])\n",
        "    n = n + 1\n",
        "  samples_new = train_x_new\n",
        "  for i in range(len(train_x_new)):\n",
        "    samples_new[i] = np.array(train_x_new[i])\n",
        "    print(samples_new[i].shape)\n",
        "  samples_new = np.array(samples_new)\n",
        "  classifications_new = np.array(train_y_new)\n",
        "  return (samples_new, classifications_new)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G1MegKuhSKU4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Split the data between master-side and patient-side manipulators\n",
        "\n",
        "**\n",
        "**\n",
        "Split a 2D numpy array of the form [[Windows],[Classification]] into a list containing two 2D numpy arrays, one for master-side data and one for patient-side data\n",
        "\n",
        "Data from each row of length 76 is split into two rows each of length 38\n"
      ]
    },
    {
      "metadata": {
        "id": "7nXgUWuIDAfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def splitMasterPatient(train_x, train_y, windowSize):\n",
        "  new_train_x = []\n",
        "  for window in train_x:\n",
        "    window_split = np.hsplit(window, 2)\n",
        "    new_train_x.append(window_split)\n",
        "  return (new_train_x)\n",
        "  new_train_y = []\n",
        "  for classification in train_y:\n",
        "    new_train_y.append([classicication, classification])\n",
        "  master_data = []\n",
        "  master_classes = []\n",
        "  patient_data = []\n",
        "  patient_classes = []\n",
        "  for i in range(len(new_train_x)):\n",
        "    master_data.append(new_train_x[i][0])\n",
        "    master_classes.append(new_train_y[i][0])\n",
        "    patient_data.append(new_train_x[i][1])\n",
        "    patient_classes.append(new_train_y[i][1])\n",
        "  master_data = np.array(master_data)\n",
        "  master_classes = np.array(master_classes)\n",
        "  patient_data = np.array(patient_data)\n",
        "  patient_classes = np.array(patient_classes)\n",
        "  return master_data + patient_data\n",
        "  #return [[master_data, master_classes], [patient_data, patient_classes]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Lnf7WM22c3X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Main"
      ]
    },
    {
      "metadata": {
        "id": "JlIfHdNAZCQR",
        "colab_type": "code",
        "outputId": "15102564-896b-4b53-c7ef-39776c9f98c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import pickle as pkl\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import stats\n",
        "from tempfile import TemporaryFile\n",
        "\n",
        "data = np.load('test.npy')\n",
        "\n",
        "# Convert numpy object back to dictionary\n",
        "data = data[()]\n",
        "# Split data for classification\n",
        "train_x_ts_data = np.array(data['Data'])\n",
        "train_y_classification = np.array(data['Class_Value'])\n",
        "#print('Features', train_x_ts_data.shape)\n",
        "#print('Class_Value', train_y_classification.shape)\n",
        "\n",
        "#printing out what the shape of the entire input is\n",
        "#samples = train_x_ts_data\n",
        "#for i in range(len(train_x_ts_data)):\n",
        "#   samples[i] = np.array(train_x_ts_data[i])\n",
        "#   print(samples[i].shape)\n",
        "#print('SAMPLE[0][0]',train_x_ts_data[0][0])\n",
        "#print('Class[0]',train_y_classification[0])\n",
        "\n",
        "#take data, Znorm\n",
        "#split into windows\n",
        "#save as npy object\n",
        "\n",
        "\n",
        "\n",
        "#print('Trials[0][0]',data[0][0])\n",
        "#print('Class[0]',train_y_classification[0])\n",
        "#print('Avgs[0]',Avgs[0])\n",
        "#print('NewData[0]',zNorm([[[1,2,3,4],[76,55,44,21]],[[93,46,102,75],[74,33,51,935]],[[0,214,241,122],[150,151,251,500]]]))\n",
        "#print('SDevs[0]',SDevs[0])\n",
        "#print('NewData:',newData)\n",
        "#print(train_y_classification)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "newX = zNorm(train_x_ts_data);\n",
        "newY=np.ndarray.tolist(train_y_classification);\n",
        "#split here into\n",
        "\n",
        "#Training trials\n",
        "trainingX = [];\n",
        "trainingY = [];\n",
        "\n",
        "for i in range(len(newX)-4):\n",
        "  trainingX.append(newX[i]);\n",
        "  trainingY.append(newY[i]);\n",
        "  \n",
        "#Testing trials\n",
        "testingX = [];\n",
        "testingY = [];\n",
        "for i in range(4):\n",
        "  testingX.append(newX[len(newX)-4+i]);\n",
        "  testingY.append(newY[len(newY)-4+i]);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "trainingY = np.asarray(trainingY);\n",
        "testingY = np.asarray(testingY);\n",
        "\n",
        "\n",
        "#print(trainingY.shape)\n",
        "#print(testingY.shape)\n",
        "#TODO split train_y_classification  \n",
        "  \n",
        "  \n",
        "(trX,trY) = jigDataSplit(trainingX, trainingY, 30, 60, 36)\n",
        "(tsX,tsY) = jigDataSplit(testingX, testingY, 30, 60, 4)\n",
        "\n",
        "\n",
        "\n",
        "#print('Features', x.shape)\n",
        "#print('Class_Value', y.shape)\n",
        "#print(y)\n",
        "\n",
        "#save (x,y) tuple into .npy objects\n",
        "outFile1 = TemporaryFile()\n",
        "np.save('output1_x', x)\n",
        "\n",
        "outFile2 = TemporaryFile()\n",
        "np.save('output1_y', y)\n",
        "\n",
        "#master = splitMasterPatient(x, y, 60)\n",
        "#print(master)\n",
        "#print(y)\n",
        "\n",
        "#https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n",
            "(60, 76)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-655d2a998e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#save (x,y) tuple into .npy objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0moutFile1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output1_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0moutFile2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTemporaryFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ErzTGEP0J_OJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from time import time\n",
        "from tensorflow.nn import softmax_cross_entropy_with_logits_v2 as categorical_cross_entropy\n",
        "from tensorflow.contrib.layers import conv2d, max_pool2d, fully_connected, flatten, dropout\n",
        "#import tensorflow.nn.softmax\n",
        "from tensorflow.losses import softmax_cross_entropy\n",
        "from tensorflow.train import AdamOptimizer\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NovWPxQvKB_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        },
        "outputId": "d379c3cc-0bf4-40df-faf7-a34db9c0cb7c"
      },
      "cell_type": "code",
      "source": [
        "data_x = trX\n",
        "data_y = trY\n",
        "train_x = np.array(data_x)\n",
        "train_y = np.array(data_y)\n",
        "\n",
        "test_x = np.array(tsX)\n",
        "test_y = np.array(tsY)\n",
        "#train_y = np.transpose(train_y)\n",
        "\n",
        "#data = data[()]\n",
        "#train_x = np.array(data['Features'])\n",
        "#train_y = np.array(data['Class_Value'])\n",
        "#print(train_x.shape)\n",
        "#print(train_y.shape)\n",
        "\n",
        "\n",
        "samples = train_x\n",
        "for i in range(len(train_x)):\n",
        "   samples[i] = np.array(train_x[i])\n",
        "   #print(samples[i].shape)\n",
        "#print(samples[2])\n",
        "\n",
        "print(type(train_x))\n",
        "print(type(train_y))\n",
        "print(train_x.shape)\n",
        "print(train_x[10].shape)\n",
        "print(train_y[1])\n",
        "print(test_x.shape)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "(81, 60, 76)\n",
            "(60, 76)\n",
            "1\n",
            "(8, 60, 76)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LyCdstAzHwg5",
        "colab_type": "code",
        "outputId": "5138cb2c-c6dc-452e-da49-fbc13fd20ae3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-20 21:17:27--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.3.53.115, 52.72.250.2, 52.54.84.112, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.3.53.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14977695 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.3’\n",
            "\n",
            "\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \rngrok-stable-linux- 100%[===================>]  14.28M  74.4MB/s    in 0.2s    \n",
            "\n",
            "2019-04-20 21:17:27 (74.4 MB/s) - ‘ngrok-stable-linux-amd64.zip.3’ saved [14977695/14977695]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "http://5866461f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IKvJGw6TKUvW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "CNN based from \"Deep LEarning with Convolutional Neural Network for Objective Skill Evaluation\n",
        "in Robot-assisted Surgery\" by Ziheng Wang and Ann Majewicz Fey\n",
        "10 January 2018 \n",
        "arXiv: 1806.05796v1\n",
        "'''\n",
        "tf.reset_default_graph()\n",
        "\n",
        "class CNN_Wang_Fey(object):\n",
        "  def __init__ (self, trainer):\n",
        "    self.X = tf.placeholder(tf.float32, shape = [None, None, None])#60, 76]) # W x C where W is length and C is channels\n",
        "    self.Y = tf.placeholder(tf.int64, shape= [None,])\n",
        "    MSM_X = tf.slice(self.X, [0, 0, 0], [-1, 60, 38])\n",
        "    PSM_X = tf.slice(self.X, [0, 0, 38], [-1, 60, 38])\n",
        "    X_r = tf.reshape(PSM_X, [-1, 60, 38, 1]) \n",
        "    #Y_r = tf.reshape(self.Y, [-1, 1])\n",
        "    Y_one_hot = tf.one_hot(self.Y, 3)\n",
        "    C1 = tf.contrib.layers.conv2d(inputs=X_r, num_outputs=38, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M1 = tf.contrib.layers.max_pool2d(inputs=C1, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D1 = tf.contrib.layers.dropout(inputs=M1, keep_prob=0.8)\n",
        "    C2 = tf.contrib.layers.conv2d(inputs=D1, num_outputs=76, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M2 = tf.contrib.layers.max_pool2d(inputs=C2, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D2 = tf.contrib.layers.dropout(inputs=M2, keep_prob=0.8)\n",
        "    C3 = tf.contrib.layers.conv2d(inputs=D2, num_outputs=152, kernel_size=(2, 2), stride=1, data_format = 'NHWC')\n",
        "    M3 = tf.contrib.layers.max_pool2d(inputs=C3, kernel_size=[2, 2], stride=2, data_format = 'NHWC')\n",
        "    D2 = tf.contrib.layers.dropout(inputs=M3, keep_prob=0.8)\n",
        "    \n",
        "    F1 = fully_connected(inputs=tf.contrib.layers.flatten(D2), num_outputs=64, activation_fn = tf.nn.softmax)\n",
        "    FD1 = tf.contrib.layers.dropout(inputs=F1, keep_prob=0.5)\n",
        "    F2 = fully_connected(inputs=FD1, num_outputs = 32)\n",
        "    FD2 = tf.contrib.layers.dropout(inputs=F2, keep_prob=0.5)\n",
        "    self.Out = fully_connected(inputs=FD2, num_outputs = 3, activation_fn = tf.nn.softmax)\n",
        "    \n",
        "    tf.summary.histogram(\"softmax\", self.Out)\n",
        "    self.loss = tf.losses.softmax_cross_entropy(Y_one_hot, self.Out)\n",
        "    tf.summary.scalar('loss', self.loss)\n",
        "    local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "    self.gradients = tf.gradients(self.loss, local_vars)\n",
        "    self.apply_grads = trainer.apply_gradients(zip(self.gradients, local_vars))\n",
        "    \n",
        "    self.accuracy =  tf.reduce_mean(tf.cast(\n",
        "            tf.equal(tf.argmax(self.Out, 1), tf.argmax(Y_one_hot, 1)), \n",
        "            tf.float32))\n",
        "    tf.summary.scalar('accuracy', self.accuracy)\n",
        "\n",
        "  \n",
        "opt = AdamOptimizer()\n",
        "net = CNN_Wang_Fey(opt)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4XHTb7WoKZMM",
        "colab_type": "code",
        "outputId": "e7ebb2f3-f919-4f20-b674-1403949bc47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1472
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "with tf.Session() as sess:\n",
        "    merge = tf.summary.merge_all()\n",
        "    now = datetime.now()\n",
        "    train_writer = tf.summary.FileWriter( './log/' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/train\", sess.graph)\n",
        "    test_writer = tf.summary.FileWriter('./log/' + now.strftime(\"%Y%m%d-%H%M%S\") + \"/test\")\n",
        "    \n",
        "    # Run the initializer\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "   \n",
        "    training_epochs = 1000\n",
        "    display_step = 1\n",
        "    # Fit all training data\n",
        "    for epoch in range(training_epochs):\n",
        "      \n",
        "      summary, _ = sess.run([merge, net.apply_grads], feed_dict={net.X: train_x, net.Y: train_y})\n",
        "      train_writer.add_summary(summary, epoch)\n",
        "       \n",
        "      # Display logs per epoch step\n",
        "      if (epoch+1) % display_step == 0:\n",
        "        c = sess.run(net.loss, feed_dict={net.X: train_x, net.Y: train_y})\n",
        "        acc = sess.run(net.accuracy, feed_dict={net.X: train_x, net.Y: train_y})\n",
        "        \n",
        "        summary_test, acc2 = sess.run([merge, net.accuracy], feed_dict={net.X: test_x, net.Y:test_y})\n",
        "        test_writer.add_summary(summary_test, epoch)\n",
        "        print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{%f}\" % c, \"accuracy=\", \"{%f}\" % acc, \"test_accuracy\", \"{%f}\" % test_acc) \n",
        "    \n",
        "\n",
        "    print(\"Optimization Finished!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= {1.097611} accuracy= {0.308642} test_accuracy {0.250000}\n",
            "Epoch: 0002 cost= {1.094059} accuracy= {0.395062} test_accuracy {0.250000}\n",
            "Epoch: 0003 cost= {1.089488} accuracy= {0.456790} test_accuracy {0.250000}\n",
            "Epoch: 0004 cost= {1.086224} accuracy= {0.493827} test_accuracy {0.250000}\n",
            "Epoch: 0005 cost= {1.082440} accuracy= {0.543210} test_accuracy {0.250000}\n",
            "Epoch: 0006 cost= {1.088322} accuracy= {0.481481} test_accuracy {0.250000}\n",
            "Epoch: 0007 cost= {1.080337} accuracy= {0.493827} test_accuracy {0.250000}\n",
            "Epoch: 0008 cost= {1.072341} accuracy= {0.518519} test_accuracy {0.250000}\n",
            "Epoch: 0009 cost= {1.073201} accuracy= {0.493827} test_accuracy {0.250000}\n",
            "Epoch: 0010 cost= {1.063090} accuracy= {0.555556} test_accuracy {0.250000}\n",
            "Epoch: 0011 cost= {1.067182} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0012 cost= {1.076887} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0013 cost= {1.072052} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0014 cost= {1.063938} accuracy= {0.543210} test_accuracy {0.250000}\n",
            "Epoch: 0015 cost= {1.084977} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0016 cost= {1.054760} accuracy= {0.543210} test_accuracy {0.250000}\n",
            "Epoch: 0017 cost= {1.067786} accuracy= {0.518519} test_accuracy {0.250000}\n",
            "Epoch: 0018 cost= {1.075836} accuracy= {0.506173} test_accuracy {0.250000}\n",
            "Epoch: 0019 cost= {1.053268} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0020 cost= {1.063404} accuracy= {0.543210} test_accuracy {0.250000}\n",
            "Epoch: 0021 cost= {1.076190} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0022 cost= {1.042365} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0023 cost= {1.075667} accuracy= {0.518519} test_accuracy {0.250000}\n",
            "Epoch: 0024 cost= {1.061750} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0025 cost= {1.047007} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0026 cost= {1.050577} accuracy= {0.518519} test_accuracy {0.250000}\n",
            "Epoch: 0027 cost= {1.049475} accuracy= {0.543210} test_accuracy {0.250000}\n",
            "Epoch: 0028 cost= {1.047367} accuracy= {0.555556} test_accuracy {0.250000}\n",
            "Epoch: 0029 cost= {1.052699} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0030 cost= {1.037580} accuracy= {0.567901} test_accuracy {0.250000}\n",
            "Epoch: 0031 cost= {1.055990} accuracy= {0.567901} test_accuracy {0.250000}\n",
            "Epoch: 0032 cost= {1.047513} accuracy= {0.555556} test_accuracy {0.250000}\n",
            "Epoch: 0033 cost= {1.055089} accuracy= {0.567901} test_accuracy {0.250000}\n",
            "Epoch: 0034 cost= {1.025439} accuracy= {0.592593} test_accuracy {0.250000}\n",
            "Epoch: 0035 cost= {1.031203} accuracy= {0.555556} test_accuracy {0.250000}\n",
            "Epoch: 0036 cost= {1.051896} accuracy= {0.530864} test_accuracy {0.250000}\n",
            "Epoch: 0037 cost= {1.021307} accuracy= {0.555556} test_accuracy {0.250000}\n",
            "Epoch: 0038 cost= {1.032212} accuracy= {0.518519} test_accuracy {0.250000}\n",
            "Epoch: 0039 cost= {1.010088} accuracy= {0.567901} test_accuracy {0.250000}\n",
            "Epoch: 0040 cost= {1.025218} accuracy= {0.592593} test_accuracy {0.250000}\n",
            "Epoch: 0041 cost= {1.034143} accuracy= {0.567901} test_accuracy {0.250000}\n",
            "Epoch: 0042 cost= {1.001990} accuracy= {0.567901} test_accuracy {0.250000}\n",
            "Epoch: 0043 cost= {1.022389} accuracy= {0.604938} test_accuracy {0.250000}\n",
            "Epoch: 0044 cost= {0.998139} accuracy= {0.580247} test_accuracy {0.250000}\n",
            "Epoch: 0045 cost= {1.002019} accuracy= {0.580247} test_accuracy {0.250000}\n",
            "Epoch: 0046 cost= {1.014964} accuracy= {0.543210} test_accuracy {0.250000}\n",
            "Epoch: 0047 cost= {0.985700} accuracy= {0.617284} test_accuracy {0.250000}\n",
            "Epoch: 0048 cost= {1.016174} accuracy= {0.543210} test_accuracy {0.250000}\n",
            "Epoch: 0049 cost= {1.015722} accuracy= {0.617284} test_accuracy {0.250000}\n",
            "Epoch: 0050 cost= {0.982554} accuracy= {0.641975} test_accuracy {0.250000}\n",
            "Epoch: 0051 cost= {1.015934} accuracy= {0.555556} test_accuracy {0.250000}\n",
            "Epoch: 0052 cost= {0.980579} accuracy= {0.592593} test_accuracy {0.250000}\n",
            "Epoch: 0053 cost= {1.007505} accuracy= {0.617284} test_accuracy {0.250000}\n",
            "Epoch: 0054 cost= {0.993138} accuracy= {0.580247} test_accuracy {0.250000}\n",
            "Epoch: 0055 cost= {0.994556} accuracy= {0.617284} test_accuracy {0.250000}\n",
            "Epoch: 0056 cost= {0.997274} accuracy= {0.641975} test_accuracy {0.250000}\n",
            "Epoch: 0057 cost= {0.975525} accuracy= {0.592593} test_accuracy {0.250000}\n",
            "Epoch: 0058 cost= {0.992973} accuracy= {0.592593} test_accuracy {0.250000}\n",
            "Epoch: 0059 cost= {0.982384} accuracy= {0.703704} test_accuracy {0.250000}\n",
            "Epoch: 0060 cost= {0.980882} accuracy= {0.617284} test_accuracy {0.250000}\n",
            "Epoch: 0061 cost= {0.997976} accuracy= {0.679012} test_accuracy {0.250000}\n",
            "Epoch: 0062 cost= {0.966758} accuracy= {0.654321} test_accuracy {0.250000}\n",
            "Epoch: 0063 cost= {0.993825} accuracy= {0.580247} test_accuracy {0.250000}\n",
            "Epoch: 0064 cost= {0.983172} accuracy= {0.666667} test_accuracy {0.250000}\n",
            "Epoch: 0065 cost= {0.987568} accuracy= {0.592593} test_accuracy {0.250000}\n",
            "Epoch: 0066 cost= {0.977830} accuracy= {0.654321} test_accuracy {0.250000}\n",
            "Epoch: 0067 cost= {0.998054} accuracy= {0.629630} test_accuracy {0.250000}\n",
            "Epoch: 0068 cost= {0.989775} accuracy= {0.641975} test_accuracy {0.250000}\n",
            "Epoch: 0069 cost= {0.950792} accuracy= {0.629630} test_accuracy {0.250000}\n",
            "Epoch: 0070 cost= {0.971673} accuracy= {0.654321} test_accuracy {0.250000}\n",
            "Epoch: 0071 cost= {0.962767} accuracy= {0.679012} test_accuracy {0.250000}\n",
            "Epoch: 0072 cost= {0.953889} accuracy= {0.666667} test_accuracy {0.250000}\n",
            "Epoch: 0073 cost= {0.963822} accuracy= {0.641975} test_accuracy {0.250000}\n",
            "Epoch: 0074 cost= {0.929005} accuracy= {0.666667} test_accuracy {0.250000}\n",
            "Epoch: 0075 cost= {0.994617} accuracy= {0.691358} test_accuracy {0.250000}\n",
            "Epoch: 0076 cost= {0.918680} accuracy= {0.703704} test_accuracy {0.250000}\n",
            "Epoch: 0077 cost= {0.967193} accuracy= {0.703704} test_accuracy {0.250000}\n",
            "Epoch: 0078 cost= {0.939109} accuracy= {0.641975} test_accuracy {0.250000}\n",
            "Epoch: 0079 cost= {0.940065} accuracy= {0.703704} test_accuracy {0.250000}\n",
            "Epoch: 0080 cost= {0.965877} accuracy= {0.679012} test_accuracy {0.250000}\n",
            "Epoch: 0081 cost= {0.947511} accuracy= {0.641975} test_accuracy {0.250000}\n",
            "Epoch: 0082 cost= {0.975920} accuracy= {0.666667} test_accuracy {0.250000}\n",
            "Epoch: 0083 cost= {0.948802} accuracy= {0.691358} test_accuracy {0.250000}\n",
            "Epoch: 0084 cost= {0.932670} accuracy= {0.641975} test_accuracy {0.250000}\n",
            "Epoch: 0085 cost= {0.978428} accuracy= {0.641975} test_accuracy {0.250000}\n",
            "Epoch: 0086 cost= {0.934763} accuracy= {0.703704} test_accuracy {0.250000}\n",
            "Epoch: 0087 cost= {0.921268} accuracy= {0.716049} test_accuracy {0.250000}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SBfTTkDzjZ1B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}